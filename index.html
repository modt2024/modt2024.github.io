<!DOCTYPE html>
<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Mixture of Diffusion Timesteps for Multimodal Generation</title>
    <style>
        body, td, th, tr, p, a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }
        
        h1 {
            font-weight: normal;
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
            margin-bottom: 15px;
        }

        h2 {
            font-weight: normal;
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
            padding-left: 20px;
        }

        h3 {
            padding-left: 40px;
            text-align: center;
        }

        p{
            padding-left: 40px;
        }
        
        .container {
            margin-left: auto;
            margin-right: auto;
            width: 80% /* Fixed width for larger screens */
            height: 960px;
            max-width: 960px;
            padding: 20px;
        }

        .content {
            font-size: 16px; 
        }
        
        .video-pair {
            display: flex;
            justify-content: space-around;
            margin-bottom: 20px;
        }
        
        .video-container {
            width: 80%; /* Adjust this value to make the video smaller */
            max-width: 200px; /* Adjust this to control the maximum size */
            margin: auto; /* Center the video container */
            position: relative;
            height: auto; /* Adjust height automatically based on the video */
            border-radius: 15px;
            overflow: hidden;
        }
        
        .video {
            width: 100%; /* Fill the container */
            height: auto; /* Adjust height automatically */
            border-radius: 15px;
            object-fit: contain; /* Fit the video within the container without cropping */
        }

        .buttons {
            text-align: center;
            padding: 20px;
            margin-top: 10px;
            margin-bottom: 15px;
        }
        
        .button {
            background-color: darkgrey;
            color: white;
            border: none;
            border-radius: 20px;
            padding: 10px 20px;
            text-decoration: none;
            margin: 0 10px;
        }
        
        .button:hover {
            background-color: grey;
        }
        
        section {
            margin-bottom: 35px; 
        }

        @media (max-width: 600px) {
            .container {
                width: 95%; /* More width on smaller screens */
            }
            
            .video-container {
                flex: 0 0 100%; /* Each video takes full width on smaller screens */
            }
        }
        
    </style>
</head>
<body>
    <div class="container">
        <h1 class="name">Mixture of Diffusion Timesteps for Multimodal Generation</h1>
        <div class="buttons">
            <a href="link-to-paper" class="button">Paper</a>
            <a href="link-to-arxiv" class="button">ArXiv</a>
        </div>
        <section>
        <div class="video-container">
            <video class="video" controls>
                <source src="test.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        </section>
        <section>
            <h2>Abstract</h2>
            <p>We present a diffusion framework that can learn a wide range of conditional distributions within multimodal data using a single model. Our key contribution lies in how we parameterize the diffusion timestep in the forward diffusion process. Instead of the canonical fixed diffusion timestep, we propose applying variable timesteps across the temporal dimension and across modalities of the inputs. This formulation offers flexibility to introduce variable noise levels for various portions of the input by using a mixture of diffusion timesteps during training, enabling a single diffusion model to learn arbitrary conditional distributions. We implement this method for multimodal video generation by developing a transformer-based audiovisual latent diffusion model. Extensive experiments demonstrate that our approach can tackle a variety of crossmodal and multimodal interpolation tasks. In particular, our approach surpasses other baselines at generating samples that are temporally and perceptually consistent with the conditioning input.</p>
        </section>
    </div>
</body>
</html>
